name: "Llamafile"
version: "1.0.0"
slug: "llamafile"
description: "Run language models locally with llamafile"
arch:
  - aarch64
  - amd64
  - armhf
  - armv7
  - i386
init: false
panel_icon: "mdi:robot"
ports:
  8080/tcp: 8080
ports_description:
  8080: "Web interface and API"
map:
  - config:rw
  - ssl:rw
  - media:rw
options:
  threads:
    type: integer
    description: "Number of CPU threads to use"
    required: false
    default: 4
schema:
  threads: int?
startup: application
boot: auto
image: "mercuryin/{arch}-addon-llamafile" 